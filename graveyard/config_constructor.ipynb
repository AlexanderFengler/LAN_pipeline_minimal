{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7b9564-be61-46d4-9e86-8cc013f99654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing lanfactory\n",
      "importing ssms\n",
      "importing lanfactory\n",
      "importing ssms\n"
     ]
    }
   ],
   "source": [
    "# Append system path to include the config scripts\n",
    "import sys\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "print('importing lanfactory')\n",
    "import lanfactory\n",
    "\n",
    "print('importing ssms')\n",
    "import ssms\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from config import *\n",
    "\n",
    "import torch\n",
    "import config\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52b4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8227d6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NETWORK_TYPE': 'cpn',\n",
       " 'CPU_BATCH_SIZE': 128,\n",
       " 'GPU_BATCH_SIZE': 512,\n",
       " 'GENERATOR_APPROACH': 'lan',\n",
       " 'OPTIMIZER_': 'adam',\n",
       " 'N_EPOCHS': 20,\n",
       " 'PROJECT_FOLDER': '/users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal/',\n",
       " 'MODEL': 'ddm',\n",
       " 'layer_sizes': [[100, 100, 100, 1],\n",
       "  [100, 100, 100, 100, 1],\n",
       "  [100, 100, 100, 100, 100, 1],\n",
       "  [120, 120, 120, 1],\n",
       "  [120, 120, 120, 120, 1],\n",
       "  [120, 120, 120, 120, 120, 1]],\n",
       " 'activations': ['NETWORK_TYPE']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml.safe_load(open('network_training_config_af.yaml', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4a1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09fa27e8-98c2-4b9a-aace-e8dab0aab2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ddm', 'ddm_legacy', 'ddm_deadline', 'angle', 'weibull', 'levy', 'levy_angle', 'full_ddm', 'gamma_drift', 'gamma_drift_angle', 'ds_conflict_drift', 'ds_conflict_drift_angle', 'ornstein', 'ornstein_angle', 'ddm_sdv', 'race_3', 'race_no_bias_3', 'race_no_bias_angle_3', 'race_4', 'race_no_bias_4', 'race_no_bias_angle_4', 'lca_3', 'lca_no_bias_3', 'lca_no_bias_angle_3', 'lca_4', 'lca_no_bias_4', 'lca_no_bias_angle_4', 'ddm_par2', 'ddm_par2_no_bias', 'ddm_par2_conflict_gamma_no_bias', 'ddm_par2_angle_no_bias', 'ddm_par2_weibull_no_bias', 'ddm_seq2', 'ddm_seq2_no_bias', 'ddm_seq2_conflict_gamma_no_bias', 'ddm_seq2_angle_no_bias', 'ddm_seq2_weibull_no_bias', 'ddm_mic2_adj', 'ddm_mic2_adj_no_bias', 'ddm_mic2_adj_conflict_gamma_no_bias', 'ddm_mic2_adj_angle_no_bias', 'ddm_mic2_adj_weibull_no_bias', 'tradeoff_no_bias', 'tradeoff_angle_no_bias', 'tradeoff_weibull_no_bias', 'tradeoff_conflict_gamma_no_bias', 'glob', 'weibull_cdf', 'full_ddm2'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssms.config.model_config.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9cfcf-8457-457b-8d83-4cd25f857bcc",
   "metadata": {},
   "source": [
    "# DATA GENERATOR CONFIGS\n",
    "\n",
    "**Note:** \n",
    "\n",
    "Look into the [ssms]() package documentation to get a better idea about the kinds of configs that you need for different kinds of training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe715a1c-fcef-4223-901a-5866ffa35039",
   "metadata": {},
   "source": [
    "### Define Metadata\n",
    "\n",
    "**Note:**\n",
    "\n",
    "ALL CAPITAL letter variables should be user specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c39eb60-efb9-4249-b7e8-4c8bd82bc6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify model\n",
    "# String name of a model which is accessible through the ssms package \n",
    "# (You can of course also use your own simulators, just make sure they are wrapped or designed in such a way\n",
    "# that their returns match the style of the ssms training data generators)\n",
    "MODEL = 'ddm' #'ddm_deadline' # 'ds_conflict_drift' \n",
    "\n",
    "# Specify data_generator parameters\n",
    "N_SAMPLES = 20000 #200000 # How many samples per parameter set?\n",
    "N_PARAMETER_SETS = 1000 #5000 # How many parameter sets in a single call?\n",
    "DELTA_T = 0.001 # Delta_t to apply to the data (simulator speed depends on this)\n",
    "N_TRAINING_SAMPLES_BY_PARAMETER_SET = 2000 # How many training samples to return for a single parameter set (Note that this is a different number from the number of samples to simulate per parameter set)\n",
    "N_SUBRUNS = 20 # How many subruns to split the training data into (This is useful if you want to run the training on multiple machines in parallel)\n",
    "CPN_ONLY = False # if true simulator will run faster but produce only choice probabilities\n",
    "\n",
    "# proj folder\n",
    "PROJECT_FOLDER = '/users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal/' # Your project base folder (other folders are going to be generated automatically in here)\n",
    "\n",
    "# What kind of likelihood approximator are we generating training data for?\n",
    "generator_approach = 'lan' if CPN_ONLY is False else 'cpn_only' # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3407e4-83a4-40be-8b66-d3eeac08447d",
   "metadata": {},
   "source": [
    "### Use Metadata to specify arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e843d7-5682-402e-b393-d7ae9fbe09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data folder\n",
    "training_data_folder = PROJECT_FOLDER + \\\n",
    "                         '/data/training_data/' + generator_approach + \\\n",
    "                            '/training_data_n_samples' + '_' + \\\n",
    "                                str(N_SAMPLES) + '/' + MODEL + '/'\n",
    "\n",
    "# Where do you want to save the config file?\n",
    "config_save_folder = PROJECT_FOLDER + '/data/config_files/data_generation/' + \\\n",
    "                        generator_approach + '/' + MODEL + '/'\n",
    "\n",
    "# Specify arguments which you want to adjust in the data generator\n",
    "data_generator_arg_dict = {\n",
    "                           'output_folder': training_data_folder,\n",
    "                           'dgp_list': MODEL,\n",
    "                           'n_samples': N_SAMPLES,\n",
    "                           'n_parameter_sets': N_PARAMETER_SETS,\n",
    "                           'delta_t': DELTA_T,\n",
    "                           'n_training_samples_by_parameter_set': N_TRAINING_SAMPLES_BY_PARAMETER_SET,\n",
    "                           'n_subruns': N_SUBRUNS,\n",
    "                           'cpn_only': CPN_ONLY,\n",
    "                          }\n",
    "\n",
    "# model_config_arg_dict = {'param_bounds': [[-2.5, 0.2, 0.1, 0.0],\n",
    "#                                           [2.5, 2.2, 0.9, 2.0]]}\n",
    "model_config_arg_dict = {}\n",
    "\n",
    "# Name of the config file\n",
    "data_config_save_name = 'nsim_' + str(data_generator_arg_dict['n_samples']) + \\\n",
    "                        '_dt_' + str(data_generator_arg_dict['delta_t']) + \\\n",
    "                        '_nps_' + str(data_generator_arg_dict['n_parameter_sets']) + \\\n",
    "                        '_npts_' + str(data_generator_arg_dict['n_training_samples_by_parameter_set']) + \\\n",
    "                        '.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce27325-2edf-4d5a-ad7b-bbd598e3f980",
   "metadata": {},
   "source": [
    "### Generate the Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c538e456-3069-4e2f-a017-9c06c5f8e4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found folder:  /users\n",
      "Moving on...\n",
      "Found folder:  /users/afengler\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data/proj_lan_pipeline\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data/proj_lan_pipeline/LAN_scripts\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data/proj_lan_pipeline/LAN_scripts/data\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data/proj_lan_pipeline/LAN_scripts/data/config_files\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data/proj_lan_pipeline/LAN_scripts/data/config_files/data_generation\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data/proj_lan_pipeline/LAN_scripts/data/config_files/data_generation/lan\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data/proj_lan_pipeline/LAN_scripts/data/config_files/data_generation/lan/ddm\n",
      "Moving on...\n",
      "Saved to: \n",
      "/users/afengler/data/proj_lan_pipeline/LAN_scripts//data/config_files/data_generation/lan/ddm/nsim_20000_dt_0.001_nps_1000_npts_2000.pickle\n"
     ]
    }
   ],
   "source": [
    "data_config_dict = make_data_generator_configs(model = MODEL,\n",
    "                                               generator_approach = generator_approach,\n",
    "                                               data_generator_arg_dict = data_generator_arg_dict,\n",
    "                                               model_config_arg_dict = model_config_arg_dict,\n",
    "                                               save_name = data_config_save_name,\n",
    "                                               save_folder = config_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e56029b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_dict': {'model_config': {'name': 'ddm',\n",
       "   'params': ['v', 'a', 'z', 't'],\n",
       "   'param_bounds': [[-3.0, 0.3, 0.1, 0.0], [3.0, 2.5, 0.9, 2.0]],\n",
       "   'boundary': <function ssms.basic_simulators.boundary_functions.constant(t=0)>,\n",
       "   'n_params': 4,\n",
       "   'default_params': [0.0, 1.0, 0.5, 0.001],\n",
       "   'hddm_include': ['z'],\n",
       "   'nchoices': 2},\n",
       "  'data_config': {'output_folder': '/users/afengler/data/proj_lan_pipeline/LAN_scripts//data/training_data/cpn_only/training_data_n_samples_20000/ddm/',\n",
       "   'dgp_list': 'ddm',\n",
       "   'n_samples': 20000,\n",
       "   'n_parameter_sets': 1000,\n",
       "   'n_parameter_sets_rejected': 100,\n",
       "   'n_training_samples_by_parameter_set': 2000,\n",
       "   'max_t': 20.0,\n",
       "   'delta_t': 0.001,\n",
       "   'pickleprotocol': 4,\n",
       "   'n_cpus': 'all',\n",
       "   'negative_rt_cutoff': -66.77497,\n",
       "   'n_subruns': 20,\n",
       "   'cpn_only': True}},\n",
       " 'config_file_name': '/users/afengler/data/proj_lan_pipeline/LAN_scripts//data/config_files/data_generation/cpn_only/ddm/nsim_20000_dt_0.001_nps_1000_npts_2000.pickle'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e579d-1f16-4abb-81fa-9d4252d8ec69",
   "metadata": {},
   "source": [
    "# NETWORK AND TRAIN CONFIGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f88cbc-3ab9-48f2-813f-0fdbd7de580e",
   "metadata": {},
   "source": [
    "### Define Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b588ae2b-613f-4a75-96ac-608c6954c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network type\n",
    "NETWORK_TYPE = 'cpn' # lan, choicep\n",
    "\n",
    "# Backend dependent batch size (note that batch sizes for cpns can be much smaller than the ones for LANs)\n",
    "# Data is passed to CPNs parameter-wise, whiel for LANs it is passed 'trial-wise' (inflating the total amount of training data for LANs by a factor of ~ 1000)\n",
    "CPU_BATCH_SIZE = 128 # lan: 1000, cpn: 128  # Batch size if cpu backend is detected (note that training success can very well depend on batch size, this parameter doesn't just affect the training speed)\n",
    "GPU_BATCH_SIZE = 512 # lan: 50000, cpn: 1024 # Batch size if gpu backend is detected\n",
    "\n",
    "# What kind of training data?\n",
    "# Note: If you have `lan` data you can also train a `cpn` on it\n",
    "# The option is here to allow for training a `cpn` on `cpn` data only, since this is somewhat cheaper to produce\n",
    "GENERATOR_APPROACH = 'lan'\n",
    "\n",
    "# Which optimizer to choose?\n",
    "OPTIMIZER_ = 'adam' # 'adam', 'sgd'\n",
    "\n",
    "# How many epochs to train?\n",
    "N_EPOCHS = 20 # reasonable defaults --> lan: 20, cpn: 50\n",
    "\n",
    "# Where do you want to save config files?\n",
    "network_train_config_save_folder = PROJECT_FOLDER + '/data/config_files/network/' + \\\n",
    "                                       NETWORK_TYPE + '/' + MODEL + '/'\n",
    "\n",
    "\n",
    "\n",
    "# Specify training data folder:\n",
    "training_data_folder = PROJECT_FOLDER + \\\n",
    "                           '/data/training_data/' + GENERATOR_APPROACH + '/' + \\\n",
    "                               'training_data_n_samples' +  \\\n",
    "                                '_' + str(N_SAMPLES) + '/' + MODEL\n",
    "\n",
    "# Specify the name of the config file\n",
    "network_train_config_save_name = 'train_config' + \\\n",
    "                                     '_opt_' + OPTIMIZER_ + \\\n",
    "                                        '_n_' + str(N_SAMPLES) + \\\n",
    "                                            '_dt_' + str(DELTA_T) + \\\n",
    "                                                '_nps_' + str(N_PARAMETER_SETS) + \\\n",
    "                                                    '_npts_' + str(N_TRAINING_SAMPLES_BY_PARAMETER_SET) + \\\n",
    "                                                        '_architecture_search.pickle'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train output type specifies what the network output node\n",
    "# 'represents' (e.g. log-probabilities / logprob, logits, probabilities / prob)\n",
    "\n",
    "# Specifically for cpn, we train on logit outputs for numerical stability, then transform outputs\n",
    "# to log-probabilities when running the model in evaluation / inference mode \n",
    "train_output_type_dict = {'lan': 'logprob',\n",
    "                          'cpn': 'logits',\n",
    "                          'cpn_bce': 'prob'}\n",
    "\n",
    "# Last layer activation depending on train output type\n",
    "output_layer_dict = {'logits': 'linear',\n",
    "                     'logprob': 'linear',\n",
    "                     'prob': 'sigmoid'}\n",
    "\n",
    "# LOSS \n",
    "# 'bce' (for binary-cross-entropy), use when train output is 'prob'\n",
    "# 'bcelogit' (for binary-cross-entropy with inputs representing logits) use when train output type is 'logits', (this is standard for cpns)\n",
    "# 'huber' (usually) used when train output is 'logprob'\n",
    "\n",
    "train_loss_dict = {'logprob': 'huber',\n",
    "                   'logits': 'bcelogit',\n",
    "                   'prob': 'bce'}\n",
    "\n",
    "# Network architectures\n",
    "layer_sizes = [[100, 100, 100, 1], [100, 100, 100, 100, 1], [100, 100, 100, 100, 100, 1],\n",
    "               [120, 120, 120, 1], [120, 120, 120, 120, 1], [120, 120, 120, 120, 120, 1]] #,\n",
    "#                [150, 150, 150, 1], [150, 150, 150, 150, 1], [150, 150, 150, 150, 150, 1]]\n",
    "\n",
    "activations = [['tanh', 'tanh', 'tanh', output_layer_dict[train_output_type_dict[NETWORK_TYPE]]], \n",
    "               ['tanh', 'tanh', 'tanh', 'tanh', output_layer_dict[train_output_type_dict[NETWORK_TYPE]]], \n",
    "               ['tanh', 'tanh', 'tanh', 'tanh', 'tanh', output_layer_dict[train_output_type_dict[NETWORK_TYPE]]],\n",
    "               ['tanh', 'tanh', 'tanh', output_layer_dict[train_output_type_dict[NETWORK_TYPE]]], \n",
    "               ['tanh', 'tanh', 'tanh', 'tanh', output_layer_dict[train_output_type_dict[NETWORK_TYPE]]], \n",
    "               ['tanh', 'tanh', 'tanh', 'tanh', 'tanh', output_layer_dict[train_output_type_dict[NETWORK_TYPE]]]\n",
    "              ]\n",
    "\n",
    "weight_decays = [0.0]\n",
    "\n",
    "# Train / validations split\n",
    "train_val_split = [0.98, 0.98, 0.98,\n",
    "                   0.98, 0.98, 0.98]\n",
    "\n",
    "# Number is set to 10000 here (an upper bound), for training on all available data (usually roughly 300 files, but has never been more than 1000)\n",
    "# For numerical experiments, one may want to artificially constraint the number of training files to teest the impact on network performance\n",
    "n_training_files = [10000]\n",
    "\n",
    "# \n",
    "network_arg_dict = {'train_output_type': train_output_type_dict[NETWORK_TYPE],\n",
    "                    'network_type': NETWORK_TYPE}\n",
    "\n",
    "\n",
    "data_key_dict = {'lan': {'features_key': 'data', \n",
    "                         'label_key': 'labels'},\n",
    "                 'cpn': {'features_key': 'thetas',\n",
    "                         'label_key': 'choice_p'}\n",
    "                }\n",
    "\n",
    "# initial train_arg_dict\n",
    "# refined in for loop in next cell\n",
    "train_arg_dict = {'n_epochs': N_EPOCHS,\n",
    "                  'loss': train_loss_dict[train_output_type_dict[NETWORK_TYPE]],\n",
    "                  'optimizer': OPTIMIZER_,\n",
    "                  'train_output_type': train_output_type_dict[NETWORK_TYPE],\n",
    "                  # 'n_training_files': n_training_files[j],\n",
    "                  # 'train_val_split': train_val_split[i],\n",
    "                  'cpu_batch_size': CPU_BATCH_SIZE,\n",
    "                  'gpu_batch_size': GPU_BATCH_SIZE,\n",
    "                  'shuffle_files': True,\n",
    "                  'label_lower_bound': np.log(1e-7),\n",
    "                  # 'weight_decay': weight_decays[k],\n",
    "                  'learning_rate': 0.001,\n",
    "                  'features_key': data_key_dict[NETWORK_TYPE]['features_key'],\n",
    "                  'label_key': data_key_dict[NETWORK_TYPE]['label_key'],\n",
    "                  'save_history': True,\n",
    "                  'lr_scheduler': 'reduce_on_plateau',\n",
    "                  'lr_scheduler_params': {'factor': 0.1,\n",
    "                                          'patience': 2,\n",
    "                                          'threshold': 0.001,\n",
    "                                          'min_lr': 0.00000001,\n",
    "                                          'verbose': True,\n",
    "                                         }\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e67d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_config = yaml.safe_load(open('network_training_config_af.yaml', 'rb'))['cpn']\n",
    "\n",
    "\n",
    "# Where do you want to save config files?\n",
    "network_train_config_save_folder = basic_config['PROJECT_FOLDER'] + '/data/config_files/network/' + \\\n",
    "                                       basic_config['NETWORK_TYPE'] + '/' + basic_config['MODEL'] + '/'\n",
    "\n",
    "\n",
    "# Specify training data folder:\n",
    "training_data_folder = basic_config['PROJECT_FOLDER'] + \\\n",
    "                           '/data/training_data/' + basic_config['GENERATOR_APPROACH'] + '/' + \\\n",
    "                               'training_data_n_samples' +  \\\n",
    "                                '_' + str(N_SAMPLES) + '/' + basic_config['MODEL']\n",
    "\n",
    "# Specify the name of the config file\n",
    "network_train_config_save_name = 'train_config' + \\\n",
    "                                     '_opt_' + basic_config['OPTIMIZER_'] + \\\n",
    "                                        '_n_' + str(N_SAMPLES) + \\\n",
    "                                            '_dt_' + str(DELTA_T) + \\\n",
    "                                                '_nps_' + str(N_PARAMETER_SETS) + \\\n",
    "                                                    '_npts_' + str(N_TRAINING_SAMPLES_BY_PARAMETER_SET) + \\\n",
    "                                                        '_architecture_search.pickle'\n",
    "\n",
    "# Train output type specifies what the network output node\n",
    "# 'represents' (e.g. log-probabilities / logprob, logits, probabilities / prob)\n",
    "\n",
    "# Specifically for cpn, we train on logit outputs for numerical stability, then transform outputs\n",
    "# to log-probabilities when running the model in evaluation / inference mode \n",
    "train_output_type_dict = {'lan': 'logprob',\n",
    "                          'cpn': 'logits',\n",
    "                          'cpn_bce': 'prob'}\n",
    "\n",
    "# Last layer activation depending on train output type\n",
    "output_layer_dict = {'logits': 'linear',\n",
    "                     'logprob': 'linear',\n",
    "                     'prob': 'sigmoid'}\n",
    "\n",
    "# LOSS \n",
    "# 'bce' (for binary-cross-entropy), use when train output is 'prob'\n",
    "# 'bcelogit' (for binary-cross-entropy with inputs representing logits) use when train output type is 'logits', (this is standard for cpns)\n",
    "# 'huber' (usually) used when train output is 'logprob'\n",
    "\n",
    "train_loss_dict = {'logprob': 'huber',\n",
    "                   'logits': 'bcelogit',\n",
    "                   'prob': 'bce'}\n",
    "\n",
    "data_key_dict = {'lan': {'features_key': 'data', \n",
    "                         'label_key': 'labels'},\n",
    "                 'cpn': {'features_key': 'thetas',\n",
    "                         'label_key': 'choice_p'}\n",
    "                }\n",
    "\n",
    "# Network architectures\n",
    "layer_sizes = basic_config['layer_sizes']\n",
    "\n",
    "\n",
    "activations = basic_config['activations']\n",
    "\n",
    "# Append last layer (type of layer depends on type of network as per train_output_type_dict dictionary above)\n",
    "activations = [act_tmp.append(output_layer_dict[train_output_type_dict[basic_config['NETWORK_TYPE']]]) for act_tmp in activations]\n",
    "\n",
    "# Train / validations split\n",
    "train_val_split = basic_config['train_val_split']\n",
    "weight_decay = basic_config['weight_decay']\n",
    "# train_val_split = [0.98, 0.98, 0.98,\n",
    "#                    0.98, 0.98, 0.98]\n",
    "\n",
    "# Number is set to 10000 here (an upper bound), for training on all available data (usually roughly 300 files, but has never been more than 1000)\n",
    "# For numerical experiments, one may want to artificially constraint the number of training files to teest the impact on network performance\n",
    "n_training_files = basic_config['n_training_files']\n",
    "\n",
    "network_arg_dict = {'train_output_type': train_output_type_dict[basic_config['NETWORK_TYPE']],\n",
    "                    'network_type': basic_config['NETWORK_TYPE']}\n",
    "\n",
    "# initial train_arg_dict\n",
    "# refined in for loop in next cell\n",
    "train_arg_dict_new = {'n_epochs': basic_config['N_EPOCHS'],\n",
    "                      'loss': train_loss_dict[train_output_type_dict[basic_config['NETWORK_TYPE']]],\n",
    "                      'optimizer': basic_config['OPTIMIZER_'],\n",
    "                      'train_output_type': train_output_type_dict[basic_config['NETWORK_TYPE']],\n",
    "                      'n_training_files': n_training_files,\n",
    "                      'train_val_split': train_val_split,\n",
    "                      'weight_decay': weight_decay,\n",
    "                      'cpu_batch_size': basic_config['CPU_BATCH_SIZE'],\n",
    "                      'gpu_batch_size': basic_config['GPU_BATCH_SIZE'],\n",
    "                      'shuffle_files': basic_config['SHUFFLE'],\n",
    "                      'label_lower_bound': eval(basic_config['labels_lower_bound']),\n",
    "                      # 'weight_decay': weight_decays[k],\n",
    "                      'learning_rate': basic_config['learning_rate'],\n",
    "                      'features_key': data_key_dict[basic_config['NETWORK_TYPE']]['features_key'],\n",
    "                      'label_key': data_key_dict[basic_config['NETWORK_TYPE']]['label_key'],\n",
    "                      'save_history': True,\n",
    "                      'lr_scheduler': basic_config['lr_scheduler'],\n",
    "                      'lr_scheduler_params': basic_config['lr_scheduler_params']\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a13ca716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NETWORK_TYPE': 'cpn',\n",
       " 'CPU_BATCH_SIZE': 128,\n",
       " 'GPU_BATCH_SIZE': 512,\n",
       " 'GENERATOR_APPROACH': 'lan',\n",
       " 'OPTIMIZER_': 'adam',\n",
       " 'N_EPOCHS': 20,\n",
       " 'PROJECT_FOLDER': '/users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal/',\n",
       " 'MODEL': 'ddm',\n",
       " 'layer_sizes': [[100, 100, 100, 1],\n",
       "  [100, 100, 100, 100, 1],\n",
       "  [100, 100, 100, 100, 100, 1],\n",
       "  [120, 120, 120, 1],\n",
       "  [120, 120, 120, 120, 1],\n",
       "  [120, 120, 120, 120, 120, 1]],\n",
       " 'activations': [['tanh', 'tanh', 'tanh', 'linear'],\n",
       "  ['tanh', 'tanh', 'tanh', 'tanh', 'linear'],\n",
       "  ['tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'linear'],\n",
       "  ['tanh', 'tanh', 'tanh', 'linear'],\n",
       "  ['tanh', 'tanh', 'tanh', 'tanh', 'linear'],\n",
       "  ['tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'linear']],\n",
       " 'weight_decays': [0.0],\n",
       " 'train_val_split': [0.98, 0.98, 0.98, 0.98, 0.98, 0.98],\n",
       " 'n_training_files': [10000],\n",
       " 'labels_lower_bound': 'np.log(1e-7)',\n",
       " 'learning_rate': 0.001,\n",
       " 'lr_scheduler': 'reduce_on_plateau',\n",
       " 'lr_scheduler_params': {'factor': '0.1,',\n",
       "  'patience': '2,',\n",
       "  'threshold': 0.001,\n",
       "  'min_lr': 1e-08,\n",
       "  'verbose': True}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8d868e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_arg_dict == train_arg_dict_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcfc7c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_epochs': 20,\n",
       " 'loss': 'bcelogit',\n",
       " 'optimizer': 'adam',\n",
       " 'train_output_type': 'logits',\n",
       " 'cpu_batch_size': 128,\n",
       " 'gpu_batch_size': 512,\n",
       " 'shuffle_files': True,\n",
       " 'label_lower_bound': -16.11809565095832,\n",
       " 'learning_rate': 0.001,\n",
       " 'features_key': 'thetas',\n",
       " 'label_key': 'choice_p',\n",
       " 'save_history': True,\n",
       " 'lr_scheduler': 'reduce_on_plateau',\n",
       " 'lr_scheduler_params': {'factor': 0.1,\n",
       "  'patience': 2,\n",
       "  'threshold': 0.001,\n",
       "  'min_lr': 1e-08,\n",
       "  'verbose': True}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_arg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46a86e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_epochs': 20,\n",
       " 'loss': 'bcelogit',\n",
       " 'optimizer': 'adam',\n",
       " 'train_output_type': 'logits',\n",
       " 'cpu_batch_size': 128,\n",
       " 'gpu_batch_size': 512,\n",
       " 'shuffle_files': True,\n",
       " 'label_lower_bound': -16.11809565095832,\n",
       " 'learning_rate': 0.001,\n",
       " 'features_key': 'thetas',\n",
       " 'label_key': 'choice_p',\n",
       " 'save_history': True,\n",
       " 'lr_scheduler': 'reduce_on_plateau',\n",
       " 'lr_scheduler_params': {'factor': '0.1,',\n",
       "  'patience': '2,',\n",
       "  'threshold': 0.001,\n",
       "  'min_lr': 1e-08,\n",
       "  'verbose': True}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_arg_dict_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d0407-aa53-40e5-b2db-f7535cd6117a",
   "metadata": {},
   "source": [
    "### Create the Config file\n",
    "\n",
    "\n",
    "Here you actually create the config file for a network training run. \n",
    "\n",
    "This will be created as a dictionary with high level keys `[0, 1, 2, ...]`, to facilitate batch jobs that index into the specific key and run training\n",
    "on the respective network.\n",
    "\n",
    "Under each numerical `key` you will find a `network_config` and a `train_config` which jointly specify the metadata for a training run.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "Network training runs pre-suppose training data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1bd45b-1d18-4df8-8f59-bfc49ffebe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total,  6  different networks will be trained with this config file\n",
      "Now saving\n",
      "Found folder:  /users\n",
      "Moving on...\n",
      "Found folder:  /users/afengler\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data\n",
      "Moving on...\n",
      "Found folder:  /users/afengler/data/proj_lan_pipeline\n",
      "Moving on...\n",
      "Did not find folder:  /users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal\n",
      "Creating it...\n",
      "Did not find folder:  /users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal/data\n",
      "Creating it...\n",
      "Did not find folder:  /users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal/data/config_files\n",
      "Creating it...\n",
      "Did not find folder:  /users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal/data/config_files/network\n",
      "Creating it...\n",
      "Did not find folder:  /users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal/data/config_files/network/cpn\n",
      "Creating it...\n",
      "Did not find folder:  /users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal/data/config_files/network/cpn/ddm\n",
      "Creating it...\n"
     ]
    }
   ],
   "source": [
    "# Loop objects\n",
    "config_dict = {}\n",
    "train_arg_dict = train_arg_dict_new\n",
    "\n",
    "cnt = 0\n",
    "for i in range(len(layer_sizes)):    \n",
    "    # Specify the arguments which you want to adjust in the network and train configs\n",
    "    # For details check: lanfactory.config.network_config_mlp\n",
    "    #                    lanfactory.config.train_config_mlp\n",
    "\n",
    "    network_arg_dict['layer_sizes'] = layer_sizes[i]\n",
    "    network_arg_dict['activations'] = activations[i]\n",
    "    \n",
    "    #train_arg_dict['n_training_files'] = n_training_files[j]\n",
    "    #train_arg_dict['train_val_split'] = train_val_split[i]\n",
    "    #train_arg_dict['weight_decay'] = weight_decays[k]\n",
    "\n",
    "    config_dict[cnt] = make_train_network_configs(training_data_folder=training_data_folder,\n",
    "                                                  save_folder = network_train_config_save_folder,\n",
    "                                                  train_val_split=train_val_split,\n",
    "                                                  network_arg_dict = deepcopy(network_arg_dict),\n",
    "                                                  train_arg_dict = deepcopy(train_arg_dict),\n",
    "                                                  save_name = None)\n",
    "\n",
    "    cnt += 1\n",
    "\n",
    "print('In total, ',\n",
    "          len(list(config_dict.keys())),\n",
    "              ' different networks will be trained with this config file')\n",
    "\n",
    "print('Now saving')\n",
    "\n",
    "# Create save_folder if not already there\n",
    "lanfactory.utils.try_gen_folder(folder = network_train_config_save_folder,\n",
    "                                allow_abs_path_folder_generation = True)\n",
    "                \n",
    "# pickle.dump(config_dict, \n",
    "#             open(network_train_config_save_folder + network_train_config_save_name, 'wb'))\n",
    "# print(network_train_config_save_folder + network_train_config_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97eec6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_epochs': 50,\n",
       " 'loss': 'bcelogit',\n",
       " 'optimizer': 'adam',\n",
       " 'train_output_type': 'logits',\n",
       " 'n_training_files': 10000,\n",
       " 'train_val_split': 0.98,\n",
       " 'weight_decay': 0.0,\n",
       " 'cpu_batch_size': 128,\n",
       " 'gpu_batch_size': 512,\n",
       " 'shuffle_files': True,\n",
       " 'label_lower_bound': -16.11809565095832,\n",
       " 'learning_rate': 0.001,\n",
       " 'features_key': 'thetas',\n",
       " 'label_key': 'choice_p',\n",
       " 'save_history': True,\n",
       " 'lr_scheduler': 'reduce_on_plateau',\n",
       " 'lr_scheduler_params': {'factor': 0.1,\n",
       "  'patience': 2,\n",
       "  'threshold': 0.001,\n",
       "  'min_lr': 1e-08,\n",
       "  'verbose': True}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_arg_dict_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22268d98-a711-431d-93a4-1b872da863a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_dict': {'network_config': {'layer_sizes': [100, 100, 100, 1],\n",
       "   'activations': None,\n",
       "   'train_output_type': 'logits',\n",
       "   'network_type': 'cpn'},\n",
       "  'train_config': {'cpu_batch_size': 128,\n",
       "   'gpu_batch_size': 512,\n",
       "   'n_epochs': 50,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.001,\n",
       "   'lr_scheduler': 'reduce_on_plateau',\n",
       "   'lr_scheduler_params': {'factor': 0.1,\n",
       "    'patience': 2,\n",
       "    'threshold': 0.001,\n",
       "    'min_lr': 1e-08,\n",
       "    'verbose': True},\n",
       "   'weight_decay': 0.0,\n",
       "   'loss': 'bcelogit',\n",
       "   'save_history': True,\n",
       "   'train_output_type': 'logits',\n",
       "   'n_training_files': 10000,\n",
       "   'train_val_split': 0.98,\n",
       "   'shuffle_files': True,\n",
       "   'label_lower_bound': -16.11809565095832,\n",
       "   'features_key': 'thetas',\n",
       "   'label_key': 'choice_p'},\n",
       "  'training_data_folder': '/users/afengler/data/proj_lan_pipeline/LAN_pipeline_minimal//data/training_data/lan/training_data_n_samples_20000/ddm',\n",
       "  'train_val_split': 0.98},\n",
       " 'config_file_name': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f7d1af0-a2fa-4da3-878e-35edb3a4f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pickle.load(open(network_train_config_save_folder + network_train_config_save_name, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lan_pipe",
   "language": "python",
   "name": "lan_pipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
